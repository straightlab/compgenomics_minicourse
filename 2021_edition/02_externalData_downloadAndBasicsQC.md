# Intro to long read data

## Goals
In this part, we're going to download some sequencing data from the Gene Expression Omnibus https://www.ncbi.nlm.nih.gov/geo/, which is a public genomics data repository . For the purpose of familiarizing ourselves with accessing, downloading, and analyzing data from other groups, we will use a dataset derived from mouse sperm cells. https://www.ncbi.nlm.nih.gov/sra?term=SRX9822381



Covered tools and concepts in this notebook:
- fastq files
- use basic commands to view files `cat`,`head` and `less`
- use `wc` to count lines and very simple `awk` scripts to perform line by line operations on a file
- use `sort` and `uniq` to create histograms
- chain linux commands with `|` and redirect output to a file with `>`
- run a command line program such as `fastqc` to process the data in some manner
- download data from Sherlock to your local machine with `rsync`

## GEO data download 
Data stored on GEO can be downloaded using the `fasterq-dump` tool.
We'll download the raw data within the subfolder `raw` within the folder `/data/external` we created earlier. Always keep things organized to avoid headaches down the road!

```bash
#recall your folder name
export me="teamStraight"

#move to the folder for this dataset
cd $GROUP_SCRATCH/biochem_minicourse_2021/$me/data/external

#make a new directory for the raw data
mkdir raw

# download the data (note that we use the tmp folder created earlier for the temporary files generated by this program)
Note: the following command takes a bit of time, so we've already downloaded the file into our directory. Use the cp command to copy from our dir to yours (this is the first 100000 lines from the full fastq file).
fasterq-dump SRR13403380 --progress --threads 2 --temp $GROUP_SCRATCH/biochem_minicourse_2021/$me/tmp/ --outdir ./raw

cp $GROUP_SCRATCH/biochem_minicourse_2021/teamStraight/data/external/raw/SRR13403380_head.fastq ./raw
```

Let's do some quick inspection of the data we downloaded.
```
ls -lh raw
```
We can see the previous command just downloaded a file called `SRR13403380.fastq` wich is 940Mb in size

This is a fastq formatted file, which is the standard format for sequencing data. We can figure out how the file is organized by looking at the first 10 lines.

```
head raw/SRR13403380_head.fastq
```

This is of the form 
```text
<@READID (first read) and some other read info>
<DNA sequence>
<+repeat of the READID line>
<Base call quality>
<@READID (next read) and some other read info>
etc..
```
So each read takes up 4 lines, with the sequence for read 1 at line2, sequence for read2 at line 4+2=6, etc... 

## Basic qc 
### The quick manual way (optional)
A lot of basic analysis of sequencing data can be done without relying on external software. It is extremely useful to know a few built-in linux commands that will come in handy in many contexts and especially for parsing raw sequencing data. 4 commands that are used often are `wc`, `sort`, `awk`, `uniq` (if you want to learn more commands you should also look at `cut`, `join` and `paste`). Let's demonstrate a real world application of these commands by doing the following qc analysis:

- how many reads are present in our dataset
- what is the read length distribution

We can see how many reads are in this file by counting the number of lines and dividing by 4

```
wc -l raw/SRR13403380_subset.fastq
# 100000 SRR13403380_subset.fastq
# 100000/4 = 250000 reads

#we can do the same for the full file
wc -l /scratch/groups/astraigh/biochem_minicourse_2021/teamStraight/data/external/raw/SRR13403380.fastq
```

What is the length of each read? We need to count the number of characters for each sequence line. First let's  extract the sequences from the file. `awk` is a great unix program to manipulate text files line by line. Here we just filter lines with a number modulo 4 = 2 (line numbers divided by 4 that give a remainder of 2). 
Let's extract the first 3 sequences (3 multiplied by 4 lines = `head -n12`). The pipe operator `|` allows us to chain commands. The parenthesis in `awk` serves as an if statement. `$0` is a variable containing the whole line.

```
head -n12 raw/SRR13403380_subset.fastq | awk '(NR%4==2){print $0}'
```

Now to get the length of these first three sequences, we just print the lengh of the line instead of the line itself

```
head -n12 raw/SRR13403380_SRR13403380.fastq | awk '(NR%4==2){print length($0)}'
```

Finally, we want to build a histogram of how many times each read length is represented. For that, we need two useful commands: sort and uniq. Sort will just sort the lines, and uniq (which requires sorted input), will count (with flag `-c`) how many times each unique line occurs. We're ready to do that for the whole file rather than the first three sequences so let's replace `head` with `cat`, which just reads through the whole file

```
cat raw/SRR13403380_subset.fastq | awk '(NR%4==2){print length($0)}' | sort | uniq -c 
```

We can redirect the output to a file with the `>` operator. Let's call this file `readlength.hist.txt` and put it in a dedicated `qc` folder. One last command we may want to add to the chain of commands is another `sort` command so that the histogram is sorted in such a way that the mode of the distribution comes first (the length that shows up the most often). This is achieved with 

```bash
# make a directory for the qc analysis
mkdir -p qc

#create histogram and save it into a file
cat raw/SRR13403380_subset.fastq | awk '(NR%4==2){print length($0)}' | sort | uniq -c | sort -k1,1nr > qc/readlength.hist.txt
```

We can look at this file in a scrollable way with `less`.
``` 
less qc/readlength.hist.txt
```

### QC with a dedicated tool fastqc and downloading data
The `fastqc` tool (preinstalled on the lab partition) can be used to get some other qc metrics, in particular about sequencing quality and overrepresentated sequences.

```
fastqc raw/SRR13403380_subset.fastq -o qc/ -t 2 
```

This produced an html file, which we need to download to our computer to look at. File transfer operations to and from Sherlock are best done using `rsync`

Open a new terminal tab in your computer, and then run 
```
# replace teamStraight with your folder name, and replace ~/Downloads if necessary
rsync -ah --progress <username>@dtn.sherlock.stanford.edu:/scratch/groups/astraigh/biochem_minicourse_2021/teamStraight/data/external/qc/SRR13403380_subset_fastqc.html ~/Downloads

```
The command has the form `rsync <option flags> [source path] [destination path]`.  The optional flags `-vh` and `--progress` are just to tune the behavior of rsync and tell it to display progress in a nice way and `-a` is to preserve timestamps on files. 

The source in on sherlock, and more specifically for file transfer we want to use a dedicated file transfer node on sherlock `dtn`. The target path in this example is your local Download directory.

Now let's take a look. On you local terminal, run
```
open ~/Dowloads/SRR13403380_subset_fastqc.html
```

### NanoStat
Another simple tool which produces qc metrics more relevant to long read data is `NanoStat`

```bash
NanoStat --fastq raw/SRR13403380_subset.fastq -o qc -n nanostat.summary
```

Questions:
- What is the N50 for this dataset?
- What is the median Q score?
- what is the median error rate?

